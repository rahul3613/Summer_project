{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses a recommendation system to predict the 'terms' which can be used for a given proof based on the list of 'types' used in its statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and Read JSON file's data\n",
    "\n",
    "file = open(r\"shallow-frequencies/shallow-frequencies.json\", \"r\", encoding='utf-8')\n",
    "js = file.read()\n",
    "data = json.loads(js)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the proofs\n",
    "proofs = data[\"triples\"]\n",
    "\n",
    "# Divide proofs into train & test Data \n",
    "no_test = 150    # No of elements in test data\n",
    "train, test = train_test_split(proofs, test_size = (no_test/len(proofs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the list of all types and terms to avoid any biasedness peresent in data\n",
    "\n",
    "statements = [row[\"name\"] for row in data[\"types\"]]\n",
    "random.shuffle(statements)\n",
    "proofs = [row[\"name\"] for row in data[\"terms\"]]\n",
    "random.shuffle(proofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with index beeing the list of terms & columns being the list of Types\n",
    "\n",
    "df = pd.DataFrame(index = proofs)\n",
    "\n",
    "for statement in statements:\n",
    "    df[statement] = [0]*len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the 'train' data\n",
    "for val in train:\n",
    "    df.loc[val[\"terms\"], val[\"types\"]] += 1\n",
    "    \n",
    "# Get the Correlation dataframe (to be used while getting recommendations)\n",
    "df_corr = df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return list of all 'terms' based on their decreasing order of priorities for a given list of 'types'\n",
    "\n",
    "def get_recom(statement):\n",
    "    corr = df_corr[statement]\n",
    "    corr = corr.mean(axis=1)\n",
    "    wght = df * corr\n",
    "\n",
    "    wght[\"mean\"] = wght.mean(axis=1)\n",
    "    final = wght[[\"mean\"]]\n",
    "    final = final.sort_values(by=[\"mean\"], ascending=False)\n",
    "\n",
    "    return final.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get score (average rank of terms in the recommended list)\n",
    "\n",
    "def get_score(triple, single = True):\n",
    "    if triple[\"terms\"] == [] or triple[\"types\"] == []:\n",
    "        return \"terms_or_types_is_empty\"\n",
    "    \n",
    "    else:\n",
    "        recom = get_recom(triple[\"types\"])\n",
    "\n",
    "        pos_recom = []\n",
    "        pos_norm = []\n",
    "\n",
    "        for term in triple[\"terms\"]:\n",
    "            pos_recom.append(recom.index(term))\n",
    "            pos_norm.append(df.index.to_list().index(term))\n",
    "\n",
    "        avg_recom = mean(pos_recom)\n",
    "        avg_norm = mean(pos_norm)\n",
    "        \n",
    "        if single:\n",
    "            return avg_recom\n",
    "        else:\n",
    "            return (avg_recom, avg_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the average score for more than one test data\n",
    "\n",
    "def avg_score(triples, graph = False):\n",
    "    print(f\"Calculating average score for {len(triples)} elements.\")\n",
    "    print(\"element\\t(avg_recom, avg_norm)\")\n",
    "    \n",
    "    i = 0\n",
    "    list_recom = []\n",
    "    list_index = []\n",
    "    \n",
    "    for triple in triples:\n",
    "        score = get_score(triple, False)\n",
    "        if score != \"terms_or_types_is_empty\":\n",
    "            list_recom.append(score)\n",
    "            list_index.append(score[0])\n",
    "        else:\n",
    "            list_recom.append((0, 0))\n",
    "            list_index.append(0)\n",
    "        i+=1\n",
    "        #print(f\" {i} : {score}\")\n",
    "    \n",
    "    avg_per = mean(list_index)\n",
    "    print(f\" Average rank of terms present in the proof of {len(list_recom)} elements is {round(avg_per)} out of {len(data['terms'])} total terms.\")\n",
    "    \n",
    "    return avg_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average score for 150 elements.\n",
      "element\t(avg_recom, avg_norm)\n",
      " Average rank of terms present in the proof of 150 elements is 317 out of 3337 total terms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "316.83887671212926"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-8f25085885fb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-8f25085885fb>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    466 394 487 504\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "466 394 487 504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the average score (x) represents that it only requires x% of the time (ordering) when using the recommended list to find the terms used in the proof.\n",
    "i.e. the average score is 100% if the 'terms' for a proof would have been found using normal 'terms' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the average score for more than one test data (used for plotting graph)\n",
    "\n",
    "def avg_score_graph(triples):\n",
    "    \n",
    "    list_recom = []\n",
    "    list_norm = []\n",
    "    \n",
    "    for triple in triples:\n",
    "        score = get_score(triple, False)\n",
    "        if score != \"terms_or_types_is_empty\":\n",
    "            list_recom.append(score[0])\n",
    "            list_norm.append(score[1])\n",
    "    \n",
    "    return list_recom, list_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_recom, scores_norm = avg_score_graph(test)\n",
    "test_no = list(range(1,len(scores_norm)+1))\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.plot(test_no, scores_recom, 'g*', label=\"Recommended_index\")\n",
    "plt.axhline(y = mean(scores_recom), color = 'g', ls = 'dotted', label=\"Avg_recommended_index\")\n",
    "\n",
    "plt.plot(test_no, scores_norm, 'r+', label=\"Normal_index\")\n",
    "plt.axhline(y = mean(scores_norm), color = 'r', ls = 'dotted', label=\"Avg_normal_index\")\n",
    "\n",
    "plt.axhline(y = 12, color = 'b', ls = 'dotted', label=\"Target_index\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.xlabel('Proofs')\n",
    "plt.ylabel('Index of Terms')\n",
    "\n",
    "plt.title('Index Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_gen(triple):\n",
    "    if triple[\"terms\"] == [] or triple[\"types\"] == []:\n",
    "        return \"terms_or_types_is_empty\"\n",
    "    \n",
    "    else:\n",
    "        recom = get_recom(triple[\"types\"])\n",
    "\n",
    "        pos_recom = []\n",
    "        for term in triple[\"terms\"]:\n",
    "            pos_recom.append(recom.index(term))\n",
    "\n",
    "        max_index = max(pos_recom)\n",
    "        mean_index = mean(pos_recom)\n",
    "\n",
    "        return (triple[\"name\"], triple[\"types\"], triple[\"terms\"], recom[:max_index+1], pos_recom, round(mean_index,2), max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_gen_top(triples, top):\n",
    "    list_rows = []\n",
    "    \n",
    "    for triple in triples:\n",
    "        values = table_gen(triple)\n",
    "        if values != \"terms_or_types_is_empty\":\n",
    "            list_rows.append(values)\n",
    "    \n",
    "    table_df = pd.DataFrame(list_rows, columns =['name', 'Types', 'Actual_terms', 'Top_pred_terms', 'Pos_of_actual_term_in_pred (out_of 3337 terms)', 'Avg_pred_index', 'Max_pred_index'])\n",
    "    table_df = table_df.set_index('name')\n",
    "    table_df = table_df.sort_values('Avg_pred_index')\n",
    "    \n",
    "    table_df = table_df.head(top)\n",
    "    \n",
    "    # Generate csv and json file for pretty view\n",
    "    table_df.to_csv('top_pred.csv')\n",
    "    table_df.to_json('top_pred.json', orient='index', indent=3)\n",
    "    \n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating table for best 20 (can be changed by changing top value in fun) prediction\n",
    "\n",
    "display(table_gen_top(test, top = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses a recommendation system to predict the 'terms' which can be used for a given proof based on the list of 'types' used in its statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import random\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = open(r\"shallow_deps/split15.yaml\", \"r\", encoding='utf-8')\n",
    "yaml_content = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r\"shallow_deps/def_type.json\", \"r\", encoding='utf-8')\n",
    "js = file.read()\n",
    "def_type = json.loads(js)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_test = 50\n",
    "tests = []\n",
    "random.shuffle(def_type['defn'])\n",
    "random.shuffle(def_type['types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(0, index=def_type['defn'], columns=def_type['types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    yaml_file = open(f\"shallow_deps/split{i}.yaml\", \"r\", encoding='utf-8')\n",
    "    yaml_content = yaml.safe_load(yaml_file)\n",
    "\n",
    "    train, test = train_test_split(yaml_content, test_size = (no_test/len(yaml_content)))\n",
    "    tests.extend(test)\n",
    "\n",
    "    for val in train:\n",
    "        defn1 = [str(val) for val in val[\"defn\"]]\n",
    "        types1 = [str(val) for val in val[\"type\"]]\n",
    "        df.loc[defn1, types1] += 1\n",
    "        \n",
    "    print(\"Done :\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done : 0\n",
      "Done : 1\n",
      "Done : 2\n",
      "Done : 3\n",
      "Done : 4\n",
      "Done : 5\n",
      "Done : 6\n",
      "Done : 7\n",
      "Done : 8\n",
      "Done : 9\n",
      "Done : 10\n",
      "Done : 11\n",
      "Done : 12\n",
      "Done : 13\n",
      "Done : 14\n",
      "Done : 15\n",
      "Done : 16\n",
      "Done : 17\n",
      "Done : 18\n",
      "Done : 19\n",
      "Done : 20\n",
      "Done : 21\n",
      "Done : 22\n",
      "Done : 23\n",
      "Done : 24\n",
      "Done : 25\n",
      "Done : 26\n",
      "Done : 27\n",
      "Done : 28\n",
      "Done : 29\n",
      "Done : 30\n"
     ]
    }
   ],
   "source": [
    "for i in range(31):\n",
    "    yaml_file = open(f\"shallow_deps/split{i}.yaml\", \"r\", encoding='utf-8')\n",
    "    yaml_content = yaml.safe_load(yaml_file)\n",
    "    yaml_file.close()\n",
    "\n",
    "    for val in yaml_content:\n",
    "        val[\"defn\"] = [str(val) for val in val[\"defn\"]]\n",
    "        val[\"type\"] = [str(val) for val in val[\"type\"]]\n",
    "        \n",
    "    json_file = open(f\"shallow_deps/split{i}.json\", \"w\", encoding='utf-8')\n",
    "    json.dump(yaml_content, json_file, indent=2)\n",
    "    json_file.close()\n",
    "        \n",
    "    print(\"Done :\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 3}, {'name': 3}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [{'name' : 5},{'name' : 6}]\n",
    "for a in b:\n",
    "    a['name'] = 3\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    yaml_file = open(f\"shallow_deps/split{i}.yaml\", \"r\", encoding='utf-8')\n",
    "    yaml_content = yaml.safe_load(yaml_file)\n",
    "\n",
    "    for j in range(len(yaml_content)):\n",
    "        defn = defn.union(set(yaml_content[j]['defn']))\n",
    "        types = types.union(set(yaml_content[j]['type']))\n",
    "        \n",
    "    print('Done :' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"defn\": defn1,\n",
    "        \"types\": types1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shallow_deps/def_type.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defn1 = [str(i) for i in defn]\n",
    "types1 = [str(i) for i in types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defn1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r\"shallow_deps/write.yaml\", \"r\", encoding='utf-8')\n",
    "vals = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "index = [m.start() for m in re.finditer('- name:', vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = 0\n",
    "new = 1\n",
    "name = 0\n",
    "\n",
    "for i in range(1, len(index)):\n",
    "    \n",
    "    if i%5000 == 0 or i == len(index)-1:\n",
    "        file = open(f\"shallow_deps/split{name}.yaml\", \"w\", encoding='utf-8')\n",
    "        file.write(vals[index[last]:index[i]])\n",
    "        file.close()\n",
    "        last = i\n",
    "        name += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6016.92"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "150423/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the proofs\n",
    "proof_types = []\n",
    "for proof in yaml_content:\n",
    "    string = ''\n",
    "    for term in proof['defn']:\n",
    "        string += (str(term)+' ')\n",
    "    proof_types.append(string[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='', lower=False)\n",
    "tokenizer.fit_on_texts(proof_types)\n",
    "word_index = tokenizer.word_index\n",
    "total_unique_words = len(tokenizer.word_index) + 1 \n",
    "print(total_unique_words)\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(yaml_content)):\n",
    "    yaml_content[i]['defn'] = tokenizer.texts_to_sequences([yaml_content[i]['defn']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "updated = re.sub(r'^PANIC.*\\n?', '', yaml_content, flags=re.MULTILINE)\n",
    "\n",
    "file = open(r\"shallow_deps/write.yaml\", \"w\", encoding='utf-8')\n",
    "file.write(updated)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml_content[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and Read JSON file's data\n",
    "\n",
    "file = open(r\"shallow_deps/shallow_deps.yaml\", \"r\", encoding='utf-8')\n",
    "js = file.read()\n",
    "data = json.loads(js)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the proofs\n",
    "proofs = data[\"triples\"]\n",
    "\n",
    "# Divide proofs into train & test Data \n",
    "no_test = 250    # No of elements in test data\n",
    "train, test = train_test_split(proofs, test_size = (no_test/len(proofs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the list of all types and terms to avoid any biasedness peresent in data\n",
    "\n",
    "statements = [row[\"name\"] for row in data[\"types\"]]\n",
    "random.shuffle(statements)\n",
    "proofs = [row[\"name\"] for row in data[\"terms\"]]\n",
    "random.shuffle(proofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with index beeing the list of terms & columns being the list of Types\n",
    "\n",
    "df = pd.DataFrame(index = proofs)\n",
    "\n",
    "for statement in statements:\n",
    "    df[statement] = [0]*len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the 'train' data\n",
    "for val in train:\n",
    "    df.loc[val[\"terms\"], val[\"types\"]] += 1\n",
    "    \n",
    "# Get the Correlation dataframe (to be used while getting recommendations)\n",
    "df_corr = df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return list of all 'terms' based on their decreasing order of priorities for a given list of 'types'\n",
    "\n",
    "def get_recom(statement):\n",
    "    corr = df_corr[statement]\n",
    "    corr = corr.mean(axis=1)\n",
    "    wght = df * corr\n",
    "\n",
    "    wght[\"mean\"] = wght.mean(axis=1)\n",
    "    final = wght[[\"mean\"]]\n",
    "    final = final.sort_values(by=[\"mean\"], ascending=False)\n",
    "\n",
    "    return final.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get score (average rank of terms in the recommended list)\n",
    "\n",
    "def get_score(triple, single = True):\n",
    "    if triple[\"terms\"] == [] or triple[\"types\"] == []:\n",
    "        return \"terms_or_types_is_empty\"\n",
    "    \n",
    "    else:\n",
    "        recom = get_recom(triple[\"types\"])\n",
    "\n",
    "        pos_recom = []\n",
    "        pos_norm = []\n",
    "\n",
    "        for term in triple[\"terms\"]:\n",
    "            pos_recom.append(recom.index(term))\n",
    "            pos_norm.append(df.index.to_list().index(term))\n",
    "\n",
    "        avg_recom = mean(pos_recom)\n",
    "        avg_norm = mean(pos_norm)\n",
    "        \n",
    "        if single:\n",
    "            return avg_recom\n",
    "        else:\n",
    "            return (avg_recom, avg_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the average score for more than one test data\n",
    "\n",
    "def avg_score(triples, graph = False):\n",
    "    print(f\"Calculating average score for {len(triples)} elements.\")\n",
    "    print(\"element\\t(avg_recom, avg_norm)\")\n",
    "    \n",
    "    i = 0\n",
    "    list_recom = []\n",
    "    list_index = []\n",
    "    \n",
    "    for triple in triples:\n",
    "        score = get_score(triple, False)\n",
    "        if score != \"terms_or_types_is_empty\":\n",
    "            list_recom.append(score)\n",
    "            list_index.append(score[0])\n",
    "        i+=1\n",
    "        #print(f\" {i} : {score}\")\n",
    "    \n",
    "    avg_per = mean(list_index)\n",
    "    print(f\" Average rank of terms present in the proof of {len(list_recom)} elements is {round(avg_per)} out of {len(data['terms'])} total terms.\")\n",
    "    \n",
    "    return avg_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the average score (x) represents that it only requires x% of the time (ordering) when using the recommended list to find the terms used in the proof.\n",
    "i.e. the average score is 100% if the 'terms' for a proof would have been found using normal 'terms' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the average score for more than one test data (used for plotting graph)\n",
    "\n",
    "def avg_score_graph(triples):\n",
    "    \n",
    "    list_recom = []\n",
    "    list_norm = []\n",
    "    \n",
    "    for triple in triples:\n",
    "        score = get_score(triple, False)\n",
    "        if score != \"terms_or_types_is_empty\":\n",
    "            list_recom.append(score[0])\n",
    "            list_norm.append(score[1])\n",
    "    \n",
    "    return list_recom, list_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_recom, scores_norm = avg_score_graph(test)\n",
    "test_no = list(range(1,len(scores_norm)+1))\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.plot(test_no, scores_recom, 'g*', label=\"Recommended_index\")\n",
    "plt.axhline(y = mean(scores_recom), color = 'g', ls = 'dotted', label=\"Avg_recommended_index\")\n",
    "\n",
    "plt.plot(test_no, scores_norm, 'r+', label=\"Normal_index\")\n",
    "plt.axhline(y = mean(scores_norm), color = 'r', ls = 'dotted', label=\"Avg_normal_index\")\n",
    "\n",
    "plt.axhline(y = 12, color = 'b', ls = 'dotted', label=\"Target_index\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.xlabel('Proofs')\n",
    "plt.ylabel('Index of Terms')\n",
    "\n",
    "plt.title('Index Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_gen(triple):\n",
    "    if triple[\"terms\"] == [] or triple[\"types\"] == []:\n",
    "        return \"terms_or_types_is_empty\"\n",
    "    \n",
    "    else:\n",
    "        recom = get_recom(triple[\"types\"])\n",
    "\n",
    "        pos_recom = []\n",
    "        for term in triple[\"terms\"]:\n",
    "            pos_recom.append(recom.index(term))\n",
    "\n",
    "        max_index = max(pos_recom)\n",
    "        mean_index = mean(pos_recom)\n",
    "\n",
    "        return (triple[\"name\"], triple[\"types\"], triple[\"terms\"], recom[:max_index+1], pos_recom, round(mean_index,2), max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_gen_top(triples, top):\n",
    "    list_rows = []\n",
    "    \n",
    "    for triple in triples:\n",
    "        values = table_gen(triple)\n",
    "        if values != \"terms_or_types_is_empty\":\n",
    "            list_rows.append(values)\n",
    "    \n",
    "    table_df = pd.DataFrame(list_rows, columns =['name', 'Types', 'Actual_terms', 'Top_pred_terms', 'Pos_of_actual_term_in_pred (out_of 3337 terms)', 'Avg_pred_index', 'Max_pred_index'])\n",
    "    table_df = table_df.set_index('name')\n",
    "    table_df = table_df.sort_values('Avg_pred_index')\n",
    "    \n",
    "    table_df = table_df.head(top)\n",
    "    \n",
    "    # Generate csv and json file for pretty view\n",
    "    table_df.to_csv('top_pred.csv')\n",
    "    table_df.to_json('top_pred.json', orient='index', indent=3)\n",
    "    \n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating table for best 20 (can be changed by changing top value in fun) prediction\n",
    "\n",
    "display(table_gen_top(test, top = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
